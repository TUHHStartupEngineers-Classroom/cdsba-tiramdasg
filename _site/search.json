[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Part 1\nFor each variable, compute the following values. You can use the built-in functions or use the mathematical formulas.\n\nExpected Value\nVariance\nStandard Deviation\n\n\nrandom_vars &lt;- readRDS(\"../../Causal_Data_Science_Data/random_vars.rds\")\n\n# part 1\nexpected_value_age &lt;- mean(random_vars$age)\nexpected_value_income &lt;- mean(random_vars$income)\n\nvariance_age &lt;- var(random_vars$age)\nvariance_income &lt;- var(random_vars$income)\n\nstd_dev_age &lt;- sd(random_vars$age)\nstd_dev_income &lt;- sd(random_vars$income)\n\ncat(\"Expected Values \\nAge: \", expected_value_age,\"\\nIncome: \", expected_value_income)\n\n#&gt; Expected Values \n#&gt; Age:  33.471 \n#&gt; Income:  3510.731\n\ncat(\"Variance \\nAge: \", variance_age,\"\\nIncome: \", variance_income)\n\n#&gt; Variance \n#&gt; Age:  340.6078 \n#&gt; Income:  8625646\n\ncat(\"Standard Deviation \\nAge: \", std_dev_age,\"\\nIncome: \", std_dev_income)\n\n#&gt; Standard Deviation \n#&gt; Age:  18.45556 \n#&gt; Income:  2936.945\n\n\n\n\nPart 2\nExplain, if it makes sense to compare the standard deviations.\n\nStandard deviation measures the spread or variability of a distribution. If one variable has a much larger standard deviation than another, it indicates greater variability in the data points.\nComparing standard deviations makes sense if the variables are measured in same units. In this example one is age in years and another is salary in currency. Therefore, it is not meaningful in this example to compare standard deviations directly.\n\n\n\nPart 3\nThen, examine the relationship between both variables and compute: covariance and correlation\n\nCovariance is a measure of how 2 variables vary together. If positive, it suggests that higher values of one variable are associated with higher variables of the other, and vice versa if negative covariance.\nCorrelation is a standardized measure that indicates the strength and direction of a linear relationship between 2 variables. Ranges from -1 to 1, where -1 indicates a perfect negative linear relationship, +1 indicates a perfect positive linear relationship, and 0 indicates no linear relationship.\n\n\ncovar_age_income &lt;- cov(random_vars$age, random_vars$income)\ncorr_age_income &lt;- cor(random_vars$age, random_vars$income)\n\ncat(\"Covariance: \", covar_age_income, \"\\nCorrelation: \", corr_age_income)\n\n#&gt; Covariance:  29700.15 \n#&gt; Correlation:  0.5479432\n\n\n\n\nPart 4\nWhat measure is easier to interpret? Please discuss your interpretation.\nFrom above we see from the value of Covariance is too large and hard to interpret how the variables age and income vary with respect to each other.\nAs correlation is standardized, ranging from -1 to 1, it is easier to compare the strength and direction of relationships between variables, regardless of the scale or units.\nHere, correlation is positive with a value of approximately 0.5479, implying a positive linear relationship between age and income.\n\n\nPart 5\nCompute the conditional expected values for:\n\nE[income | age &lt;= 18]\nE[income | age \\(\\in\\) [18, 65)]\nE[income | age &gt;= 65]\n\n\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nrandom_vars &lt;- random_vars %&gt;%\n  filter(!is.na(income), !is.na(age))\n\ncond_exp_income_age_18 &lt;- random_vars %&gt;%\n  filter(age &lt;= 18) %&gt;%\n  summarise(mean_income = mean(income)) %&gt;%\n  pull(mean_income)\n\ncond_exp_income_age_18_65 &lt;- random_vars %&gt;%\n  filter(age &gt;= 18, age &lt; 65) %&gt;%\n  summarise(mean_income = mean(income)) %&gt;%\n  pull(mean_income)\n\ncond_exp_income_age_65 &lt;- random_vars %&gt;%\n  filter(age &gt;= 65) %&gt;%\n  summarise(mean_income = mean(income)) %&gt;%\n  pull(mean_income)\n\ncat(\"18 and below: \", cond_exp_income_age_18, \"\\n18 and 65: \", cond_exp_income_age_18_65, \"\\n65 above: \", cond_exp_income_age_65)\n\n#&gt; 18 and below:  389.6074 \n#&gt; 18 and 65:  4685.734 \n#&gt; 65 above:  1777.237"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Spurious Correlation example.\nLoading the required libraries:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\n\nSet seed for reproducability\n\nset.seed(210)\n\nThe following code has the following:\n\nSimulation Function (simulate_ar):\n\nsimulate_ar function generates a time series of length n with an auto-regressive structure defined by the parameter \\(\\phi\\).\nThe auto-regressive process is simulated with the formula \\(Y_t = \\phi \\cdot Y_{t-1} + \\epsilon_t\\), where \\(\\epsilon_t\\) is a normally distributed random error with mean 0 and standard deviation sigma.\n\nSimulation and Testing Loop:\n\nSet up a grid of combinations for times, n, and phis using “expand.grid”.\nIterates over each combination and performs a correlation test between two time series generated with the same auto-regressive structure but different random noise.\nThe results are stored as matrix res.\n\nData Manipulation with dplyr:\n\nres converted to a data frame (dat) grouped by the phi values, and summary statistics (average absolute correlation, average absolute t-statistic, and the percentage of statistically significant p-values).\n\n\n\nn &lt;- 500\ntimes &lt;- 100\nphis &lt;- seq(0, 1, .02)\ncomb &lt;- expand.grid(times = seq(times), n = n, phis)\nncomb &lt;- nrow(comb)\n\nres &lt;- matrix(NA, nrow = ncomb, ncol = 6)\ncolnames(res) &lt;- c('ix', 'n', 'phi', 'cor', 'tstat', 'pval')\n\nsimulate_ar &lt;- function(n, phi, sigma = .1) {\n  y &lt;- rep(0, n)\n  \n  for (t in seq(2, n)) {\n    y[t] &lt;- phi*y[t-1] + rnorm(1, 0, sigma)\n  }\n  \n  y\n}\n\nfor (i in seq(ncomb)) {\n  ix &lt;- comb[i, 1]\n  n &lt;- comb[i, 2]\n  phi &lt;- comb[i, 3]\n  \n  test &lt;- cor.test(simulate_ar(n, phi = phi), simulate_ar(n, phi = phi))\n  res[i, ] &lt;- c(ix, n, phi, test$estimate, test$statistic, test$p.value)\n}\n\ndat &lt;- data.frame(res) %&gt;% \n  group_by(phi) %&gt;% \n  summarize(\n    avg_abs_corr = mean(abs(cor)),\n    avg_abs_tstat = mean(abs(tstat)),\n    percent_sig = mean(pval &lt; .05)\n  )\n\n\nggplot(dat, aes(x = phi, y = percent_sig)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Spurious Correlation: phi vs Avg Abs Correlation\",\n       x = \"X - Phi\",\n       y = \"Y - Avg Abs Correlation\") +\n  theme_minimal()\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe plot shows a scatter plot with a linear regression line, and visualizes the average absolute correlation against the phi values. It helps in understanding how the strength of the spurious correlation varies with different values of the auto-regressive parameter phi."
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "library(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(ggrepel)\n\n#&gt; Loading required package: ggplot2\n\ndf &lt;- readRDS(\"../../Causal_Data_Science_Data/rand_enc.rds\")\n\n# Part 1\niv_expl &lt;- dagify(\n  Y ~ D,\n  Y ~ U,\n  D ~ U,\n  D ~ Z,\n  exposure = \"D\",\n  latent = \"U\",\n  outcome = \"Y\",\n  coords = list(x = c(U = 1, D = 0, Y = 2, Z = -1),\n                y = c(U = 1, D = 0, Y = 0, Z = 0)),\n  labels = c(\"D\" = \"Used New Features\",\n             \"Y\" = \"Time Spent\",\n             \"U\" = \"Unobserved Characteristics\",\n             \"Z\" = \"Random Encouragement\")\n)\n\n# Plot the DAG\nggdag(iv_expl, text = TRUE) +\n  geom_dag_point(color = \"lightblue\") +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\") +\n  geom_dag_label_repel(aes(label = label)) +\n  theme(panel.background = element_rect(fill = \"black\"))"
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggplot2)\n\n# Load the data\ndata &lt;- readRDS(\"../../Causal_Data_Science_Data/membership.rds\")\n\n# Part 1\nsummary(data)\n\n#&gt;       age             sex         pre_avg_purch         card       \n#&gt;  Min.   :16.00   Min.   :0.0000   Min.   :-14.23   Min.   :0.0000  \n#&gt;  1st Qu.:29.80   1st Qu.:0.0000   1st Qu.: 51.82   1st Qu.:0.0000  \n#&gt;  Median :38.80   Median :1.0000   Median : 70.15   Median :0.0000  \n#&gt;  Mean   :40.37   Mean   :0.5038   Mean   : 70.42   Mean   :0.4232  \n#&gt;  3rd Qu.:49.20   3rd Qu.:1.0000   3rd Qu.: 88.79   3rd Qu.:1.0000  \n#&gt;  Max.   :90.00   Max.   :1.0000   Max.   :169.42   Max.   :1.0000  \n#&gt;    avg_purch     \n#&gt;  Min.   :-28.61  \n#&gt;  1st Qu.: 54.02  \n#&gt;  Median : 76.24  \n#&gt;  Mean   : 76.61  \n#&gt;  3rd Qu.: 98.54  \n#&gt;  Max.   :192.91\n\nstr(data)\n\n#&gt; tibble [10,000 × 5] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ age          : num [1:10000] 31.3 40.7 23.2 65.3 27.2 52.4 39.5 26.5 28.4 34.7 ...\n#&gt;  $ sex          : int [1:10000] 1 1 1 0 0 1 0 1 1 1 ...\n#&gt;  $ pre_avg_purch: num [1:10000] 61.1 43 18 82 61.4 ...\n#&gt;  $ card         : int [1:10000] 0 1 1 1 0 0 1 0 0 1 ...\n#&gt;  $ avg_purch    : num [1:10000] 70.8 51.4 26 124 45.1 ...\n\ncor(data)\n\n#&gt;                      age          sex pre_avg_purch        card   avg_purch\n#&gt; age           1.00000000  0.012532675   0.517506430 0.105533628 0.448632638\n#&gt; sex           0.01253267  1.000000000  -0.001221386 0.008468092 0.002181853\n#&gt; pre_avg_purch 0.51750643 -0.001221386   1.000000000 0.192333327 0.855828507\n#&gt; card          0.10553363  0.008468092   0.192333327 1.000000000 0.382352233\n#&gt; avg_purch     0.44863264  0.002181853   0.855828507 0.382352233 1.000000000\n\n# DAG\ndag &lt;- dagitty(\"dag {\n  avg_purch -&gt; card\n  age -&gt; card\n  sex -&gt; card\n  pre_avg_purch -&gt; card\n}\")\n\nplot(dag)\n\n#&gt; Plot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own."
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "Example from previous chapter (parking spots) and draw the DAG.\nIn this DAG:\n\nLocation is the treatment variable (whether the store is in the city center or outside the city).\nParkingSpots is the variable representing whether the store has parking spots or not.\nSales is the outcome variable (sales of the store).\n\nThe arrows in the DAG indicate the causal relationships between the variables. For example, there is an arrow from Location to ParkingSpots indicating that the location influences the availability of parking spots.\n\n# Load necessary packages\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks ggdag::filter(), stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Part 1 \ndag &lt;- dagitty(\"dag {\n  ParkingSpots -&gt; Sales\n  Location -&gt; ParkingSpots\n  Location -&gt; Sales\n}\")\n\nplot(dag)\n\n#&gt; Plot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\n# Draw DAG\nggdag(dag)"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(readr)\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nabtest_data &lt;- readRDS(\"../../Causal_Data_Science_Data/abtest_online.rds\")\n\n# Part 1\nsummary(abtest_data)\n\n#&gt;       ip             chatbot        previous_visit   mobile_device  \n#&gt;  Length:1000        Mode :logical   Min.   : 0.000   Mode :logical  \n#&gt;  Class :character   FALSE:496       1st Qu.: 1.000   FALSE:683      \n#&gt;  Mode  :character   TRUE :504       Median : 1.000   TRUE :317      \n#&gt;                                     Mean   : 2.021                  \n#&gt;                                     3rd Qu.: 3.000                  \n#&gt;                                     Max.   :13.000                  \n#&gt;     purchase     purchase_amount\n#&gt;  Min.   :0.000   Min.   : 0.00  \n#&gt;  1st Qu.:0.000   1st Qu.: 0.00  \n#&gt;  Median :0.000   Median : 0.00  \n#&gt;  Mean   :0.381   Mean   :13.14  \n#&gt;  3rd Qu.:1.000   3rd Qu.:27.44  \n#&gt;  Max.   :1.000   Max.   :81.35\n\nggplot(abtest_data, aes(x = chatbot, y = previous_visit, fill = chatbot)) +\n  geom_boxplot() +\n  labs(title = \"Continuous covariate - previous_visit\",\n       x = \"Chatbot\", y = \"Previous Visits\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(abtest_data, aes(x = mobile_device, fill = chatbot)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Categorical covariate - mobile_device\",\n       x = \"Mobile Device\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(abtest_data, aes(x = as.factor(purchase), fill = chatbot)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Binary outcome - purchase\",\n       x = \"Purchase\", y = \"Count\") +\n  theme_minimal()"
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Assuming frequency of visits to the hospital is an even number after treatment and is odd before the treatment.\n\n# Part 1\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nhosp_data &lt;- readRDS(\"../../Causal_Data_Science_Data/hospdd.rds\")\nhead(hosp_data)\n\n\n\n  \n\n\nsummary(hosp_data)\n\n#&gt;     hospital       frequency         month         procedure     \n#&gt;  Min.   : 1.00   Min.   :1.000   Min.   :1.000   Min.   :0.0000  \n#&gt;  1st Qu.:11.00   1st Qu.:1.000   1st Qu.:1.750   1st Qu.:0.0000  \n#&gt;  Median :23.00   Median :2.000   Median :3.500   Median :0.0000  \n#&gt;  Mean   :22.84   Mean   :2.473   Mean   :3.625   Mean   :0.2079  \n#&gt;  3rd Qu.:35.00   3rd Qu.:4.000   3rd Qu.:5.250   3rd Qu.:0.0000  \n#&gt;  Max.   :46.00   Max.   :4.000   Max.   :7.000   Max.   :1.0000  \n#&gt;      satis       \n#&gt;  Min.   :0.5468  \n#&gt;  1st Qu.:2.8888  \n#&gt;  Median :3.4975  \n#&gt;  Mean   :3.6191  \n#&gt;  3rd Qu.:4.2405  \n#&gt;  Max.   :9.7129\n\nmean_satis_treated_before &lt;- hosp_data %&gt;%\n  filter(procedure == 0, frequency %% 2 != 0) %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_satis_treated_after &lt;- hosp_data %&gt;%\n  filter(procedure == 0, frequency %% 2 == 0)  %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_satis_control_before &lt;- hosp_data %&gt;%\n  filter(procedure == 1, frequency %% 2 != 0) %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_satis_control_after &lt;- hosp_data %&gt;%\n  filter(procedure == 1, frequency %% 2 == 0)  %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\ncat(\"Mean Satisfaction for Treated Hospitals Before Treatment:\", mean_satis_treated_before, \"\\n\")\n\n#&gt; Mean Satisfaction for Treated Hospitals Before Treatment: 3.468616\n\ncat(\"Mean Satisfaction for Treated Hospitals After Treatment:\", mean_satis_treated_after, \"\\n\")\n\n#&gt; Mean Satisfaction for Treated Hospitals After Treatment: 3.380761\n\ncat(\"Mean Satisfaction for Control Hospitals Before Treatment:\", mean_satis_control_before, \"\\n\")\n\n#&gt; Mean Satisfaction for Control Hospitals Before Treatment: 4.39925\n\ncat(\"Mean Satisfaction for Control Hospitals After Treatment:\", mean_satis_control_after, \"\\n\")\n\n#&gt; Mean Satisfaction for Control Hospitals After Treatment: 4.331852"
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Assignment 1\nFrom the picture: \nWe know:\nP(S) = 0.3\nP(T) = 0.7\nP(T | S) = 0.2\nP(T | S) = 0.8\nP(T | S) = 0.6\nP(T | S) = 0.4\nReferring to \nThe solutions:\n\nP(T ∩ S) = P(S) * P(T | S)\nP(T ∩ S) = P(S) * P(T | S)\nP(T ∩ S) = P(S) * P(T | S)\nP(T ∩ S) = P(S) * P(T | S)\n\n\nS &lt;- 0.3\nTgS &lt;- 0.2 # 0.8 is Tbar given S\nTgSbar &lt;- 0.6 # 0.4 is Tbar given Sbar\n\nTandS &lt;- S * TgS\nTandSbar &lt;- (1 - S) * TgSbar\nTbarandS &lt;- S * (1 - TgS)\nTbarandSbar &lt;- (1 - S) * (1 - TgSbar)\n\nprint( paste( \"Sum: \", ( TandS + TandSbar + TbarandS + TbarandSbar ) ) )\n\n#&gt; [1] \"Sum:  1\"\n\n\n\n\nAssignment 2\n\n\n\nFigure 1: Venn Diagram\n\n\nFrom the venn diagram:\n\nThe percentage of customers using all three devices is\n\n5 or 0.5% of total customers\n\nThe percentage of customers using at least 2 devices\n\nCustomers using (Smartphones and Tablets) + (Tablet and Computer) + (Computer and Smartphone) + (all three)\n199 or 19.9% of customers\n\nThe percentage of customers using only one device\n\nCustomers using only smartphone + only Tablet + only computer\n801 or 80.1% of customers\n\n\n\n\nAssignment 3\nGiven:\n\nP(A) = 0.04\nP(B|A) = 0.97\nP(B|A) = 0.01\n\nTo find P(A|B) and P(A|B)\n\nCalculate P(B): \\[\nP(B) = P(B|A) \\cdot P(A) + P(B|\\overline{A}) \\cdot P(\\overline{A})\n\\]\n\n\\[\nP(B) = 0.97 \\cdot 0.04 + 0.01 \\cdot (1 - 0.04) \\\\\n\\]\n\\[\nP(B) = 0.0484\n\\] 2. Apply Bayes’ Theorem:\n\\[\nP(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)} \\\\\n\\]\n\\[\nP (A | B) = \\frac{0.97 \\cdot 0.04}{0.0484}\\\\\n\\]\n\\[\nP(A | B) = 0.791\n\\] Similarly, \\[\nP(\\overline{A} | B) = \\frac{P(B | \\overline{A}) \\cdot P(\\overline{A})}{P(B)} \\\\\n\\] \\[\nP (\\overline{A} | B) = \\frac{0.01 \\cdot (1 - 0.04)}{0.0484}\\\\\n\\] \\[\nP(\\overline{A} | B) = 0.209\n\\]\nNow subsituting the values to the sentence:\n“These results show that in case the alarm is triggered, there is a probability of about 79.1% that the product is flawless and a probability of about 20.9% that the product is faulty”."
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Part 1\nRead the data and check the dimensions. How many rows and how many columns does the data have? You could use e.g. the dim() command.\n\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 1\n\ncar_data &lt;- readRDS(\"../../Causal_Data_Science_Data/car_prices.rds\")\ncar_data &lt;- na.omit(car_data)\ncat(\"Dimensions of the data:\", dim(car_data), \"\\n\")\n\n#&gt; Dimensions of the data: 181 22\n\n\n\n\nPart 2\nUse appropriate commands to get a more detailed look at the data. What data types do you see? How do numbers differ from strings regarding their data type?\n\n# 2\n\nsummary(car_data) # may also use str(car_data)\n\n#&gt;   aspiration         doornumber          carbody           drivewheel       \n#&gt;  Length:181         Length:181         Length:181         Length:181        \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;  enginelocation       wheelbase        carlength        carwidth    \n#&gt;  Length:181         Min.   : 86.60   Min.   :141.1   Min.   :60.30  \n#&gt;  Class :character   1st Qu.: 94.50   1st Qu.:166.3   1st Qu.:64.00  \n#&gt;  Mode  :character   Median : 96.50   Median :173.0   Median :65.40  \n#&gt;                     Mean   : 98.21   Mean   :173.3   Mean   :65.74  \n#&gt;                     3rd Qu.:100.40   3rd Qu.:180.2   3rd Qu.:66.50  \n#&gt;                     Max.   :120.90   Max.   :208.1   Max.   :72.30  \n#&gt;    carheight       curbweight    enginetype        cylindernumber    \n#&gt;  Min.   :47.80   Min.   :1488   Length:181         Length:181        \n#&gt;  1st Qu.:52.00   1st Qu.:2122   Class :character   Class :character  \n#&gt;  Median :53.70   Median :2410   Mode  :character   Mode  :character  \n#&gt;  Mean   :53.58   Mean   :2521                                        \n#&gt;  3rd Qu.:55.50   3rd Qu.:2910                                        \n#&gt;  Max.   :59.80   Max.   :4066                                        \n#&gt;    enginesize     fuelsystem          boreratio         stroke    \n#&gt;  Min.   : 61.0   Length:181         Min.   :2.540   Min.   :2.07  \n#&gt;  1st Qu.: 98.0   Class :character   1st Qu.:3.150   1st Qu.:3.08  \n#&gt;  Median :120.0   Mode  :character   Median :3.310   Median :3.23  \n#&gt;  Mean   :127.1                      Mean   :3.325   Mean   :3.23  \n#&gt;  3rd Qu.:141.0                      3rd Qu.:3.590   3rd Qu.:3.40  \n#&gt;  Max.   :326.0                      Max.   :3.940   Max.   :4.17  \n#&gt;  compressionratio   horsepower       peakrpm        citympg     \n#&gt;  Min.   : 7.000   Min.   : 48.0   Min.   :4200   Min.   :13.00  \n#&gt;  1st Qu.: 8.500   1st Qu.: 70.0   1st Qu.:4800   1st Qu.:19.00  \n#&gt;  Median : 9.000   Median : 95.0   Median :5200   Median :24.00  \n#&gt;  Mean   : 8.848   Mean   :106.2   Mean   :5182   Mean   :24.85  \n#&gt;  3rd Qu.: 9.400   3rd Qu.:116.0   3rd Qu.:5500   3rd Qu.:30.00  \n#&gt;  Max.   :11.500   Max.   :288.0   Max.   :6600   Max.   :49.00  \n#&gt;    highwaympg        price      \n#&gt;  Min.   :16.00   Min.   : 5118  \n#&gt;  1st Qu.:25.00   1st Qu.: 7609  \n#&gt;  Median :30.00   Median : 9980  \n#&gt;  Mean   :30.48   Mean   :12999  \n#&gt;  3rd Qu.:34.00   3rd Qu.:16430  \n#&gt;  Max.   :54.00   Max.   :45400\n\nhead(car_data)\n\n\n\n  \n\n\n\nHas 22 Variables with\naspiration, doornumber, carbody, drivewheel, enginelocation, enginetype, cylindernumber, fuelsystem with character values,\nand wheelbase, carlength, carwidth, carheight, curbweight, enginesize, boreratio, stroke, compressionratio, horsepower, peakrpm, citympg, highwaympg, price with numeric values.\n\n\nPart 3\nRun a linear regression. You want to explain what factors are relevant for the pricing of a car.\n\n# 3\ncar_model &lt;- lm(price ~ doornumber + carbody + enginetype + fuelsystem + stroke + horsepower + citympg, data = car_data)\nsummary(car_model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ doornumber + carbody + enginetype + fuelsystem + \n#&gt;     stroke + horsepower + citympg, data = car_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -8922.8 -1774.7   -15.5  1466.0 16355.9 \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       4992.75    6211.58   0.804 0.422701    \n#&gt; doornumbertwo     -539.74     902.44  -0.598 0.550614    \n#&gt; carbodyhardtop   -3617.15    2267.11  -1.595 0.112551    \n#&gt; carbodyhatchback -6461.38    1753.77  -3.684 0.000312 ***\n#&gt; carbodysedan     -4771.37    1848.61  -2.581 0.010736 *  \n#&gt; carbodywagon     -6128.24    2032.18  -3.016 0.002978 ** \n#&gt; enginetypedohcv  -9993.95    4639.52  -2.154 0.032711 *  \n#&gt; enginetypel       4884.77    2028.64   2.408 0.017168 *  \n#&gt; enginetypeohc     3202.33    1323.44   2.420 0.016640 *  \n#&gt; enginetypeohcf    1565.76    1759.84   0.890 0.374935    \n#&gt; enginetypeohcv    2557.31    1602.92   1.595 0.112570    \n#&gt; fuelsystem2bbl    -153.25    1314.25  -0.117 0.907316    \n#&gt; fuelsystemmfi    -7560.37    4217.62  -1.793 0.074908 .  \n#&gt; fuelsystemmpfi   -1676.81    1515.73  -1.106 0.270251    \n#&gt; fuelsystemspdi   -6473.78    1931.46  -3.352 0.000999 ***\n#&gt; fuelsystemspfi     452.35    4125.97   0.110 0.912835    \n#&gt; stroke            -697.37    1245.17  -0.560 0.576210    \n#&gt; horsepower         173.88      16.93  10.268  &lt; 2e-16 ***\n#&gt; citympg           -166.47      96.59  -1.723 0.086716 .  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3879 on 162 degrees of freedom\n#&gt; Multiple R-squared:  0.792,  Adjusted R-squared:  0.7689 \n#&gt; F-statistic: 34.27 on 18 and 162 DF,  p-value: &lt; 2.2e-16\n\n\nWith some limited knowledge about cars, I think the factors mentioned above are relevant to a car price. I will use horsepower for further analysis.\n\n\nPart 4\nChoose one regressor and\n\nexplain what data type it is and what values it can take on\nwhat effect it has on the price and what changing the value would have as a result\nwhether its effect is statistically significant.\n\nChoosing “horsepower”\n\n# 4\n\n# data type it is and what values it can take on\nstr(car_data$horsepower)\n\n#&gt;  num [1:181] 111 111 154 102 115 110 110 110 140 160 ...\n\n# effect it has on the price and what changing the value would have as a result\nsummary(car_model)$coefficients['horsepower',]\n\n#&gt;     Estimate   Std. Error      t value     Pr(&gt;|t|) \n#&gt; 1.311579e+02 1.480934e+01 8.856434e+00 1.669974e-15\n\n# statistically significant\nsummary(car_model)$coefficients['horsepower','Pr(&gt;|t|)']\n\n#&gt; [1] 1.669974e-15\n\n\n\n\nPart 5\nAdd a variable seat_heating to the data and assign a value TRUE for all observations. You can use e.g. df %&gt;% mutate(new_variable = value). Assign it to a new object and run a regression. What coefficient do you get for the new variable seat_heating and how can you explain it?\n\n# 5\n\n# Add variable 'seat_heating' and run a regression\ncar_prices_new &lt;- car_data %&gt;%\n  mutate(seat_heating = TRUE) %&gt;%\n  na.omit()\n\n# Run a regression with the new variable\nmodel_new &lt;- lm(price ~ doornumber + carbody + enginetype + fuelsystem + stroke + horsepower + citympg + seat_heating, data = car_prices_new)\n\nsummary(car_prices_new$seat_heating)\n\n#&gt;    Mode    TRUE \n#&gt; logical     181\n\nsummary(model_new)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ doornumber + carbody + enginetype + fuelsystem + \n#&gt;     stroke + horsepower + citympg + seat_heating, data = car_prices_new)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -8922.8 -1774.7   -15.5  1466.0 16355.9 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       4992.75    6211.58   0.804 0.422701    \n#&gt; doornumbertwo     -539.74     902.44  -0.598 0.550614    \n#&gt; carbodyhardtop   -3617.15    2267.11  -1.595 0.112551    \n#&gt; carbodyhatchback -6461.38    1753.77  -3.684 0.000312 ***\n#&gt; carbodysedan     -4771.37    1848.61  -2.581 0.010736 *  \n#&gt; carbodywagon     -6128.24    2032.18  -3.016 0.002978 ** \n#&gt; enginetypedohcv  -9993.95    4639.52  -2.154 0.032711 *  \n#&gt; enginetypel       4884.77    2028.64   2.408 0.017168 *  \n#&gt; enginetypeohc     3202.33    1323.44   2.420 0.016640 *  \n#&gt; enginetypeohcf    1565.76    1759.84   0.890 0.374935    \n#&gt; enginetypeohcv    2557.31    1602.92   1.595 0.112570    \n#&gt; fuelsystem2bbl    -153.25    1314.25  -0.117 0.907316    \n#&gt; fuelsystemmfi    -7560.37    4217.62  -1.793 0.074908 .  \n#&gt; fuelsystemmpfi   -1676.81    1515.73  -1.106 0.270251    \n#&gt; fuelsystemspdi   -6473.78    1931.46  -3.352 0.000999 ***\n#&gt; fuelsystemspfi     452.35    4125.97   0.110 0.912835    \n#&gt; stroke            -697.37    1245.17  -0.560 0.576210    \n#&gt; horsepower         173.88      16.93  10.268  &lt; 2e-16 ***\n#&gt; citympg           -166.47      96.59  -1.723 0.086716 .  \n#&gt; seat_heatingTRUE       NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3879 on 162 degrees of freedom\n#&gt; Multiple R-squared:  0.792,  Adjusted R-squared:  0.7689 \n#&gt; F-statistic: 34.27 on 18 and 162 DF,  p-value: &lt; 2.2e-16\n\n\nAs seen in summary of regression model “seat_heatingTRUE” with NA is what we get - indicates that this categorical variable (seat_heating) has only one level, that is value “TRUE”. This is proved with summary statement above this."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "loading the necessary libraries and data from coupon.rds.\n\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\ndf &lt;- readRDS(\"../../Causal_Data_Science_Data/coupon.rds\")\nc0 &lt;- 60\n\n# Part1\nbw_half &lt;- c0 + c(-2, 2)\n\nInterval [58,62]\n\ndf_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw_half[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_half[2])\ndf_bw_half &lt;- bind_rows(df_bw_above, df_bw_below)\n\nDimension of data in [58, 62] interval\n\ndim(df_bw_half)\n\n#&gt; [1] 138   4\n\nmodel_bw_below &lt;- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above &lt;- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 &lt;- predict(model_bw_below, tibble(days_since_last = c0))\ny1 &lt;- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate &lt;- y1 - y0\nsprintf(\"LATE for half the bandwidth [58,62]: %.2f\", late)\n\n#&gt; [1] \"LATE for half the bandwidth [58,62]: 7.34\""
  },
  {
    "objectID": "content/01_journal/02_statistics.html#header-2",
    "href": "content/01_journal/02_statistics.html#header-2",
    "title": "Statistical Concepts",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/05_dag.html#part-1",
    "href": "content/01_journal/05_dag.html#part-1",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "Example from previous chapter (parking spots) and draw the DAG.\nIn this DAG:\n\nLocation is the treatment variable (whether the store is in the city center or outside the city).\nParkingSpots is the variable representing whether the store has parking spots or not.\nSales is the outcome variable (sales of the store).\n\nThe arrows in the DAG indicate the causal relationships between the variables. For example, there is an arrow from Location to ParkingSpots indicating that the location influences the availability of parking spots.\n\n# Load necessary packages\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks ggdag::filter(), stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Part 1 \ndag &lt;- dagitty(\"dag {\n  ParkingSpots -&gt; Sales\n  Location -&gt; ParkingSpots\n  Location -&gt; Sales\n}\")\n\nplot(dag)\n\n#&gt; Plot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\n# Draw DAG\nggdag(dag)"
  },
  {
    "objectID": "content/01_journal/05_dag.html#part-2",
    "href": "content/01_journal/05_dag.html#part-2",
    "title": "Directed Acyclic Graphs",
    "section": "Part 2",
    "text": "Part 2\nUsing lm function as linear regression model. summary to display detailed information about regression models.\n\ncust_sat &lt;- readRDS(\"../../Causal_Data_Science_Data/customer_sat.rds\")\n\n# 1. regress satisfaction on follow_ups\nmodel1 &lt;- lm(satisfaction ~ follow_ups, data = cust_sat)\nsummary(model1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = cust_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n# 2. regress satisfaction on follow_ups and account for subscription\nmodel2 &lt;- lm(satisfaction ~ follow_ups + subscription, data = cust_sat)\nsummary(model2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = cust_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08"
  },
  {
    "objectID": "content/01_journal/05_dag.html#part-3",
    "href": "content/01_journal/05_dag.html#part-3",
    "title": "Directed Acyclic Graphs",
    "section": "Part 3",
    "text": "Part 3\nBy observing both models, the coefficient for follow_ups is -3.3093 in model 1, and 2.1944 in model 2.\nThis maybe interpreted as for model 1, for each additional follow up call, satisfaction is decreasing by 3.3093 units holding subscription constant. Whereas for model 2, satisfaction increases 2.1944 units holding subscription constant.\nBy comparison the relation between satisfaction and follow up maybe influenced by subscription, i.e., presence of subscription levels (Premium and Premium+) has an additional impact on satisfaction beyond the effect of follow-up calls alone."
  },
  {
    "objectID": "content/01_journal/05_dag.html#part-4",
    "href": "content/01_journal/05_dag.html#part-4",
    "title": "Directed Acyclic Graphs",
    "section": "Part 4",
    "text": "Part 4\n\n# Not conditioning on subscription\ncustomer_sat_not_cond &lt;- ggplot(cust_sat, aes(x = follow_ups, y = satisfaction)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE)\n\n# Conditioning on subscription\ncustomer_sat_cond &lt;- ggplot(cust_sat, aes(x = follow_ups, y = satisfaction, color = subscription)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  theme(legend.position = \"right\")\n\n# Plot both\ncustomer_sat_not_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ncustomer_sat_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/09_iv.html#part-1",
    "href": "content/01_journal/09_iv.html#part-1",
    "title": "Instrumental Variables",
    "section": "",
    "text": "library(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(ggrepel)\n\n#&gt; Loading required package: ggplot2\n\ndf &lt;- readRDS(\"../../Causal_Data_Science_Data/rand_enc.rds\")\n\n# Part 1\niv_expl &lt;- dagify(\n  Y ~ D,\n  Y ~ U,\n  D ~ U,\n  D ~ Z,\n  exposure = \"D\",\n  latent = \"U\",\n  outcome = \"Y\",\n  coords = list(x = c(U = 1, D = 0, Y = 2, Z = -1),\n                y = c(U = 1, D = 0, Y = 0, Z = 0)),\n  labels = c(\"D\" = \"Used New Features\",\n             \"Y\" = \"Time Spent\",\n             \"U\" = \"Unobserved Characteristics\",\n             \"Z\" = \"Random Encouragement\")\n)\n\n# Plot the DAG\nggdag(iv_expl, text = TRUE) +\n  geom_dag_point(color = \"lightblue\") +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"white\") +\n  geom_dag_label_repel(aes(label = label)) +\n  theme(panel.background = element_rect(fill = \"black\"))"
  },
  {
    "objectID": "content/01_journal/09_iv.html#part-2",
    "href": "content/01_journal/09_iv.html#part-2",
    "title": "Instrumental Variables",
    "section": "Part 2",
    "text": "Part 2\n\n# Part 2\nmodel_biased &lt;- lm(time_spent ~ used_ftr, data = df)\nsummary(model_biased)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/09_iv.html#part-3",
    "href": "content/01_journal/09_iv.html#part-3",
    "title": "Instrumental Variables",
    "section": "Part 3",
    "text": "Part 3\n\nrandom encouragement is correlated with the used_ftr (positive correlation)\nrandom encouragement is not directly correlated with the time_spent which can be seen by correlation between used_ftr and time_spent and correlation between rand_enc and time_spent. Therefore IV estimation can be used.\n\n\ncor(df) %&gt;% round(2)\n\n#&gt;            rand_enc used_ftr time_spent\n#&gt; rand_enc       1.00     0.20       0.13\n#&gt; used_ftr       0.20     1.00       0.71\n#&gt; time_spent     0.13     0.71       1.00"
  },
  {
    "objectID": "content/01_journal/09_iv.html#part-4",
    "href": "content/01_journal/09_iv.html#part-4",
    "title": "Instrumental Variables",
    "section": "Part 4",
    "text": "Part 4\n\nlibrary(estimatr)\nmodel_iv &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = df)\nsummary(model_iv)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = df)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#part-1",
    "href": "content/01_journal/10_rdd.html#part-1",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "loading the necessary libraries and data from coupon.rds.\n\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\ndf &lt;- readRDS(\"../../Causal_Data_Science_Data/coupon.rds\")\nc0 &lt;- 60\n\n# Part1\nbw_half &lt;- c0 + c(-2, 2)\n\nInterval [58,62]\n\ndf_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw_half[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_half[2])\ndf_bw_half &lt;- bind_rows(df_bw_above, df_bw_below)\n\nDimension of data in [58, 62] interval\n\ndim(df_bw_half)\n\n#&gt; [1] 138   4\n\nmodel_bw_below &lt;- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above &lt;- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 &lt;- predict(model_bw_below, tibble(days_since_last = c0))\ny1 &lt;- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate &lt;- y1 - y0\nsprintf(\"LATE for half the bandwidth [58,62]: %.2f\", late)\n\n#&gt; [1] \"LATE for half the bandwidth [58,62]: 7.34\""
  },
  {
    "objectID": "content/01_journal/10_rdd.html#part-2",
    "href": "content/01_journal/10_rdd.html#part-2",
    "title": "Regression Discontinuity",
    "section": "Part 2",
    "text": "Part 2\n\nbw_double &lt;- c0 + c(-10, 10)\n\nInterval [50,70]\n\ndf_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw_double[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_double[2])\ndf_bw_double &lt;- bind_rows(df_bw_above, df_bw_below)\n\nDimension of data in [50, 70] interval\n\ndim(df_bw_double)\n\n#&gt; [1] 629   4\n\nmodel_bw_below &lt;- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above &lt;- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 &lt;- predict(model_bw_below, tibble(days_since_last = c0))\ny1 &lt;- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate &lt;- y1 - y0\nsprintf(\"LATE for double the bandwidth [50, 70]: %.2f\", late)\n\n#&gt; [1] \"LATE for double the bandwidth [50, 70]: 9.51\""
  },
  {
    "objectID": "content/01_journal/10_rdd.html#part-3",
    "href": "content/01_journal/10_rdd.html#part-3",
    "title": "Regression Discontinuity",
    "section": "Part 3",
    "text": "Part 3\nVisually check continuity at running variable. You can see that the confidence intervals doesn’t overlap. If they did not overlap, there is some kind of manipulation around the cut-off so, we cannot use RDD to obtain valid results.\n\ndf &lt;- readRDS(\"../../Causal_Data_Science_Data/shipping.rds\")\nc0 = 30\nlibrary(rddensity)\nrddd &lt;- rddensity(df$purchase_amount, c = c0)\nrdd_plot &lt;- rdplotdensity(rddd, df$purchase_amount, plotN = 100)"
  },
  {
    "objectID": "content/01_journal/07_matching.html#part-1",
    "href": "content/01_journal/07_matching.html#part-1",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggplot2)\n\n# Load the data\ndata &lt;- readRDS(\"../../Causal_Data_Science_Data/membership.rds\")\n\n# Part 1\nsummary(data)\n\n#&gt;       age             sex         pre_avg_purch         card       \n#&gt;  Min.   :16.00   Min.   :0.0000   Min.   :-14.23   Min.   :0.0000  \n#&gt;  1st Qu.:29.80   1st Qu.:0.0000   1st Qu.: 51.82   1st Qu.:0.0000  \n#&gt;  Median :38.80   Median :1.0000   Median : 70.15   Median :0.0000  \n#&gt;  Mean   :40.37   Mean   :0.5038   Mean   : 70.42   Mean   :0.4232  \n#&gt;  3rd Qu.:49.20   3rd Qu.:1.0000   3rd Qu.: 88.79   3rd Qu.:1.0000  \n#&gt;  Max.   :90.00   Max.   :1.0000   Max.   :169.42   Max.   :1.0000  \n#&gt;    avg_purch     \n#&gt;  Min.   :-28.61  \n#&gt;  1st Qu.: 54.02  \n#&gt;  Median : 76.24  \n#&gt;  Mean   : 76.61  \n#&gt;  3rd Qu.: 98.54  \n#&gt;  Max.   :192.91\n\nstr(data)\n\n#&gt; tibble [10,000 × 5] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ age          : num [1:10000] 31.3 40.7 23.2 65.3 27.2 52.4 39.5 26.5 28.4 34.7 ...\n#&gt;  $ sex          : int [1:10000] 1 1 1 0 0 1 0 1 1 1 ...\n#&gt;  $ pre_avg_purch: num [1:10000] 61.1 43 18 82 61.4 ...\n#&gt;  $ card         : int [1:10000] 0 1 1 1 0 0 1 0 0 1 ...\n#&gt;  $ avg_purch    : num [1:10000] 70.8 51.4 26 124 45.1 ...\n\ncor(data)\n\n#&gt;                      age          sex pre_avg_purch        card   avg_purch\n#&gt; age           1.00000000  0.012532675   0.517506430 0.105533628 0.448632638\n#&gt; sex           0.01253267  1.000000000  -0.001221386 0.008468092 0.002181853\n#&gt; pre_avg_purch 0.51750643 -0.001221386   1.000000000 0.192333327 0.855828507\n#&gt; card          0.10553363  0.008468092   0.192333327 1.000000000 0.382352233\n#&gt; avg_purch     0.44863264  0.002181853   0.855828507 0.382352233 1.000000000\n\n# DAG\ndag &lt;- dagitty(\"dag {\n  avg_purch -&gt; card\n  age -&gt; card\n  sex -&gt; card\n  pre_avg_purch -&gt; card\n}\")\n\nplot(dag)\n\n#&gt; Plot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own."
  },
  {
    "objectID": "content/01_journal/07_matching.html#part-2",
    "href": "content/01_journal/07_matching.html#part-2",
    "title": "Matching and Subclassification",
    "section": "Part 2",
    "text": "Part 2\n\n# Part 2\nmodel_naive &lt;- lm(card ~ avg_purch, data = data)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = card ~ avg_purch, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.0579 -0.3879 -0.1700  0.4529  1.0809 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -0.0209101  0.0116644  -1.793   0.0731 .  \n#&gt; avg_purch    0.0057968  0.0001401  41.375   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4566 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#part-3",
    "href": "content/01_journal/07_matching.html#part-3",
    "title": "Matching and Subclassification",
    "section": "Part 3",
    "text": "Part 3\n\nlibrary(MatchIt)\n\n\n1. (Coarsened) Exact Matching\nstep 1 matching, step 2 estimation.\n\n# Matching\ncem &lt;- matchit(card ~ avg_purch + age + sex + pre_avg_purch,\n               data = data, \n               method = 'cem', \n               estimand = 'ATE')\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ avg_purch + age + sex + pre_avg_purch, \n#&gt;     data = data, method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; avg_purch           91.1592       65.9397          0.8356     1.0590    0.2225\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; avg_purch       0.3281\n#&gt; age             0.0864\n#&gt; sex             0.0086\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; avg_purch           76.9109       75.9624          0.0314     1.0093    0.0095\n#&gt; age                 39.7058       39.6956          0.0008     0.9982    0.0022\n#&gt; sex                  0.5039        0.5039          0.0000          .    0.0000\n#&gt; pre_avg_purch       69.5090       69.8459         -0.0129     0.9967    0.0040\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; avg_purch       0.0279          0.1618\n#&gt; age             0.0085          0.1230\n#&gt; sex             0.0000          0.0000\n#&gt; pre_avg_purch   0.0133          0.1533\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.    4232. \n#&gt; Matched (ESS) 3883.44  2630.5\n#&gt; Matched       4895.    3622. \n#&gt; Unmatched      873.     610. \n#&gt; Discarded        0.       0.\n\nmatched_data_cem &lt;- match.data(cem)\n# Estimation\nmodel_cem &lt;- lm(avg_purch ~ card + age + sex + pre_avg_purch, data = matched_data_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card + age + sex + pre_avg_purch, data = matched_data_cem, \n#&gt;     weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -64.754  -9.438  -0.340   9.091  96.208 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   5.139250   0.589478   8.718  &lt; 2e-16 ***\n#&gt; card          1.284712   0.310822   4.133 3.61e-05 ***\n#&gt; age           0.018873   0.014254   1.324    0.186    \n#&gt; sex           0.634557   0.307380   2.064    0.039 *  \n#&gt; pre_avg_purch 0.998687   0.007334 136.176  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 14.18 on 8512 degrees of freedom\n#&gt; Multiple R-squared:  0.7498, Adjusted R-squared:  0.7497 \n#&gt; F-statistic:  6377 on 4 and 8512 DF,  p-value: &lt; 2.2e-16\n\n\nAlso including a Coarse Exact Matching with coarsion on cutpoints age and sex. This maybe wrong, but the graph looks pretty interesting.\n\n# Custom coarsening\n# (1) Matching\ncutpoints &lt;- list(age = seq(16, 25, 60), sex = seq(0, 1))\ncem_coars &lt;- matchit(card ~ avg_purch + age + sex + pre_avg_purch,\n                     data = data, \n                     method = 'cem', \n                     estimand = 'ATE',\n                     cutpoints = cutpoints)\n\n# Covariate balance\nsummary(cem_coars)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ avg_purch + age + sex + pre_avg_purch, \n#&gt;     data = data, method = \"cem\", estimand = \"ATE\", cutpoints = cutpoints)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; avg_purch           91.1592       65.9397          0.8356     1.0590    0.2225\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; avg_purch       0.3281\n#&gt; age             0.0864\n#&gt; sex             0.0086\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; avg_purch           76.8418       75.8941          0.0314     1.0104    0.0096\n#&gt; age                 39.6401       39.6187          0.0016     0.9966    0.0019\n#&gt; sex                  0.5057        0.5057          0.0000          .    0.0000\n#&gt; pre_avg_purch       69.6355       69.9416         -0.0117     0.9938    0.0035\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; avg_purch       0.0272          0.1615\n#&gt; age             0.0085          0.1139\n#&gt; sex             0.0000          0.0000\n#&gt; pre_avg_purch   0.0121          0.1533\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.   4232.  \n#&gt; Matched (ESS) 3854.52 2539.21\n#&gt; Matched       4916.   3591.  \n#&gt; Unmatched      852.    641.  \n#&gt; Discarded        0.      0.\n\n# Use matched data\ndf_cem_coars &lt;- match.data(cem_coars)\n\n# Plot grid\nggplot(df_cem_coars, aes(x = age, y = sex,\n                         size = weights, color = as.factor(card))) +\n  geom_point(alpha = .2) +\n  geom_abline(data.frame(y = cutpoints$sex),\n              mapping = aes(intercept = y, slope = 0), \n              linewidth = 1.5, color = \"lightblue\") +\n  geom_vline(data.frame(y = cutpoints$age),\n             mapping = aes(xintercept = y),\n             linewidth = 1.5, color = \"orange\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# (2) Estimation\nmodel_cem_coars &lt;- lm(avg_purch ~ card + age + sex + pre_avg_purch, data = matched_data_cem, weights = weights)\nsummary(model_cem_coars)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card + age + sex + pre_avg_purch, data = matched_data_cem, \n#&gt;     weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -64.754  -9.438  -0.340   9.091  96.208 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   5.139250   0.589478   8.718  &lt; 2e-16 ***\n#&gt; card          1.284712   0.310822   4.133 3.61e-05 ***\n#&gt; age           0.018873   0.014254   1.324    0.186    \n#&gt; sex           0.634557   0.307380   2.064    0.039 *  \n#&gt; pre_avg_purch 0.998687   0.007334 136.176  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 14.18 on 8512 degrees of freedom\n#&gt; Multiple R-squared:  0.7498, Adjusted R-squared:  0.7497 \n#&gt; F-statistic:  6377 on 4 and 8512 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n2. Nearest-Neighbor Matching\n\n# (1) Matching\n# replace: one-to-one or one-to-many matching\nnn &lt;- matchit(card ~ avg_purch + age + sex + pre_avg_purch,\n              data = data,\n              method = \"nearest\",\n              distance = \"mahalanobis\",\n              replace = T)\n\n# Covariate Balance\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ avg_purch + age + sex + pre_avg_purch, \n#&gt;     data = data, method = \"nearest\", distance = \"mahalanobis\", \n#&gt;     replace = T)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; avg_purch           91.1592       65.9397          0.8239     1.0590    0.2225\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; avg_purch       0.3281\n#&gt; age             0.0864\n#&gt; sex             0.0086\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; avg_purch           91.1592       90.2268          0.0305     1.0790    0.0051\n#&gt; age                 42.0331       41.9080          0.0090     1.0426    0.0039\n#&gt; sex                  0.5087        0.5087          0.0000          .    0.0000\n#&gt; pre_avg_purch       76.3938       76.0192          0.0142     1.0607    0.0035\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; avg_purch       0.0208          0.1117\n#&gt; age             0.0121          0.1025\n#&gt; sex             0.0000          0.0000\n#&gt; pre_avg_purch   0.0187          0.1094\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 1278.54    4232\n#&gt; Matched       2204.      4232\n#&gt; Unmatched     3564.         0\n#&gt; Discarded        0.         0\n\n# Use matched data\ndf_nn &lt;- match.data(nn)\n\n# (2) Estimation\nmodel_nn &lt;- lm(avg_purch ~ card + age + sex + pre_avg_purch, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card + age + sex + pre_avg_purch, data = df_nn, \n#&gt;     weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -50.689 -11.297  -1.974   8.271  87.075 \n#&gt; \n#&gt; Coefficients:\n#&gt;                Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   14.488581   0.755345  19.181   &lt;2e-16 ***\n#&gt; card           0.557825   0.404237   1.380   0.1677    \n#&gt; age            0.011981   0.016233   0.738   0.4605    \n#&gt; sex           -0.906351   0.383762  -2.362   0.0182 *  \n#&gt; pre_avg_purch  0.995764   0.008622 115.487   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 15.39 on 6431 degrees of freedom\n#&gt; Multiple R-squared:  0.7409, Adjusted R-squared:  0.7408 \n#&gt; F-statistic:  4598 on 4 and 6431 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3. Inverse Probability Weighting\n\n# (1) Propensity scores\nmodel_prop &lt;- glm(card ~ avg_purch + age + sex + pre_avg_purch,\n                  data = data,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ avg_purch + age + sex + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.8926909  0.0852342 -22.206   &lt;2e-16 ***\n#&gt; avg_purch      0.0641066  0.0016453  38.963   &lt;2e-16 ***\n#&gt; age            0.0002066  0.0019726   0.105    0.917    \n#&gt; sex            0.0350985  0.0458508   0.765    0.444    \n#&gt; pre_avg_purch -0.0487641  0.0018754 -26.001   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 11227  on 9995  degrees of freedom\n#&gt; AIC: 11237\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Add propensities to table\ndf_aug &lt;- data %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\n\n# Look at data with IPW scores\ndf_ipw %&gt;% \n  select(card, age, sex, avg_purch, pre_avg_purch, propensity, ipw)\n\n\n\n  \n\n\n# (2) Estimation\nmodel_ipw &lt;- lm(avg_purch ~ card,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -525.64  -28.55   -0.11   30.04  441.02 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5744     0.4664 164.189   &lt;2e-16 ***\n#&gt; card         -0.6361     0.6577  -0.967    0.333    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 46.6 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  9.356e-05,  Adjusted R-squared:  -6.45e-06 \n#&gt; F-statistic: 0.9355 on 1 and 9998 DF,  p-value: 0.3335"
  },
  {
    "objectID": "content/01_journal/08_did.html#part-1",
    "href": "content/01_journal/08_did.html#part-1",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Assuming frequency of visits to the hospital is an even number after treatment and is odd before the treatment.\n\n# Part 1\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nhosp_data &lt;- readRDS(\"../../Causal_Data_Science_Data/hospdd.rds\")\nhead(hosp_data)\n\n\n\n  \n\n\nsummary(hosp_data)\n\n#&gt;     hospital       frequency         month         procedure     \n#&gt;  Min.   : 1.00   Min.   :1.000   Min.   :1.000   Min.   :0.0000  \n#&gt;  1st Qu.:11.00   1st Qu.:1.000   1st Qu.:1.750   1st Qu.:0.0000  \n#&gt;  Median :23.00   Median :2.000   Median :3.500   Median :0.0000  \n#&gt;  Mean   :22.84   Mean   :2.473   Mean   :3.625   Mean   :0.2079  \n#&gt;  3rd Qu.:35.00   3rd Qu.:4.000   3rd Qu.:5.250   3rd Qu.:0.0000  \n#&gt;  Max.   :46.00   Max.   :4.000   Max.   :7.000   Max.   :1.0000  \n#&gt;      satis       \n#&gt;  Min.   :0.5468  \n#&gt;  1st Qu.:2.8888  \n#&gt;  Median :3.4975  \n#&gt;  Mean   :3.6191  \n#&gt;  3rd Qu.:4.2405  \n#&gt;  Max.   :9.7129\n\nmean_satis_treated_before &lt;- hosp_data %&gt;%\n  filter(procedure == 0, frequency %% 2 != 0) %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_satis_treated_after &lt;- hosp_data %&gt;%\n  filter(procedure == 0, frequency %% 2 == 0)  %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_satis_control_before &lt;- hosp_data %&gt;%\n  filter(procedure == 1, frequency %% 2 != 0) %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\nmean_satis_control_after &lt;- hosp_data %&gt;%\n  filter(procedure == 1, frequency %% 2 == 0)  %&gt;%\n  pull(satis) %&gt;%\n  mean()\n\ncat(\"Mean Satisfaction for Treated Hospitals Before Treatment:\", mean_satis_treated_before, \"\\n\")\n\n#&gt; Mean Satisfaction for Treated Hospitals Before Treatment: 3.468616\n\ncat(\"Mean Satisfaction for Treated Hospitals After Treatment:\", mean_satis_treated_after, \"\\n\")\n\n#&gt; Mean Satisfaction for Treated Hospitals After Treatment: 3.380761\n\ncat(\"Mean Satisfaction for Control Hospitals Before Treatment:\", mean_satis_control_before, \"\\n\")\n\n#&gt; Mean Satisfaction for Control Hospitals Before Treatment: 4.39925\n\ncat(\"Mean Satisfaction for Control Hospitals After Treatment:\", mean_satis_control_after, \"\\n\")\n\n#&gt; Mean Satisfaction for Control Hospitals After Treatment: 4.331852"
  },
  {
    "objectID": "content/01_journal/08_did.html#part-2",
    "href": "content/01_journal/08_did.html#part-2",
    "title": "Difference-in-Differences",
    "section": "Part 2",
    "text": "Part 2\ninclude group and time fixed effects in the regression, i.e. one regressor for each month and one regressor for each hospital as mentioned in the assignment.\nTo achieve a regressor for each month and a regressor for each hospital, we should use as.factor(month) + as.factor(hospital).\nThe month + hospital approach treats them as continuous variables, assuming a linear relationship, while the as.factor(month) + as.factor(hospital) approach treats them as categorical factors, allowing for separate intercepts for each level. This approach allows for more flexibility in capturing potential non-linear relationships and different intercepts for each level.\n\nmodel_1 &lt;- lm(satis ~ frequency + month + hospital, data = hosp_data)\nsummary(model_1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ frequency + month + hospital, data = hosp_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.3038 -0.6704 -0.0750  0.5787  5.7590 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  3.6273420  0.0396118  91.572  &lt; 2e-16 ***\n#&gt; frequency    0.0538983  0.0101629   5.303 1.17e-07 ***\n#&gt; month        0.0720728  0.0055854  12.904  &lt; 2e-16 ***\n#&gt; hospital    -0.0176390  0.0008716 -20.238  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.015 on 7364 degrees of freedom\n#&gt; Multiple R-squared:  0.07561,    Adjusted R-squared:  0.07523 \n#&gt; F-statistic: 200.8 on 3 and 7364 DF,  p-value: &lt; 2.2e-16\n\nmodel_2 &lt;- lm(satis ~ frequency + as.factor(month) + as.factor(hospital), data = hosp_data)\nsummary(model_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ frequency + as.factor(month) + as.factor(hospital), \n#&gt;     data = hosp_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.5176 -0.4868 -0.0163  0.4764  4.5046 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            3.302483   0.059857  55.173  &lt; 2e-16 ***\n#&gt; frequency              0.053751   0.007786   6.903 5.51e-12 ***\n#&gt; as.factor(month)2     -0.009608   0.030315  -0.317  0.75131    \n#&gt; as.factor(month)3      0.021969   0.030315   0.725  0.46867    \n#&gt; as.factor(month)4      0.349354   0.030315  11.524  &lt; 2e-16 ***\n#&gt; as.factor(month)5      0.343235   0.030315  11.322  &lt; 2e-16 ***\n#&gt; as.factor(month)6      0.348800   0.030315  11.506  &lt; 2e-16 ***\n#&gt; as.factor(month)7      0.341444   0.030315  11.263  &lt; 2e-16 ***\n#&gt; as.factor(hospital)2   0.369283   0.080360   4.595 4.39e-06 ***\n#&gt; as.factor(hospital)3   0.531657   0.082334   6.457 1.13e-10 ***\n#&gt; as.factor(hospital)4   0.239008   0.076751   3.114  0.00185 ** \n#&gt; as.factor(hospital)5  -0.187606   0.076976  -2.437  0.01483 *  \n#&gt; as.factor(hospital)6   0.422811   0.076819   5.504 3.84e-08 ***\n#&gt; as.factor(hospital)7   1.395230   0.074166  18.812  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8   0.073894   0.079201   0.933  0.35085    \n#&gt; as.factor(hospital)9  -1.541417   0.081267 -18.967  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10  1.666597   0.080193  20.782  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11  0.205881   0.079230   2.599  0.00938 ** \n#&gt; as.factor(hospital)12 -0.088643   0.081205  -1.092  0.27504    \n#&gt; as.factor(hospital)13  0.474560   0.078374   6.055 1.47e-09 ***\n#&gt; as.factor(hospital)14  0.216930   0.082367   2.634  0.00846 ** \n#&gt; as.factor(hospital)15 -0.180409   0.082498  -2.187  0.02879 *  \n#&gt; as.factor(hospital)16  1.413378   0.080158  17.632  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17  0.394071   0.083686   4.709 2.54e-06 ***\n#&gt; as.factor(hospital)18  0.152851   0.097358   1.570  0.11646    \n#&gt; as.factor(hospital)19 -1.171264   0.082334 -14.226  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20 -0.372378   0.080160  -4.645 3.45e-06 ***\n#&gt; as.factor(hospital)21  0.760720   0.084956   8.954  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22  0.327933   0.083858   3.911 9.29e-05 ***\n#&gt; as.factor(hospital)23  0.247468   0.082450   3.001  0.00270 ** \n#&gt; as.factor(hospital)24 -0.765773   0.088275  -8.675  &lt; 2e-16 ***\n#&gt; as.factor(hospital)25  0.182556   0.094751   1.927  0.05406 .  \n#&gt; as.factor(hospital)26 -0.231114   0.080218  -2.881  0.00397 ** \n#&gt; as.factor(hospital)27 -0.831217   0.077505 -10.725  &lt; 2e-16 ***\n#&gt; as.factor(hospital)28  0.249922   0.085124   2.936  0.00334 ** \n#&gt; as.factor(hospital)29 -0.230596   0.081591  -2.826  0.00472 ** \n#&gt; as.factor(hospital)30 -0.597227   0.097361  -6.134 9.01e-10 ***\n#&gt; as.factor(hospital)31  0.064165   0.080233   0.800  0.42389    \n#&gt; as.factor(hospital)32 -0.767554   0.081252  -9.447  &lt; 2e-16 ***\n#&gt; as.factor(hospital)33 -0.904455   0.080250 -11.270  &lt; 2e-16 ***\n#&gt; as.factor(hospital)34 -0.419040   0.075363  -5.560 2.79e-08 ***\n#&gt; as.factor(hospital)35 -0.069493   0.077495  -0.897  0.36988    \n#&gt; as.factor(hospital)36  1.700127   0.078341  21.702  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37 -0.301118   0.094608  -3.183  0.00146 ** \n#&gt; as.factor(hospital)38 -0.501452   0.079212  -6.331 2.59e-10 ***\n#&gt; as.factor(hospital)39 -0.465018   0.083615  -5.561 2.77e-08 ***\n#&gt; as.factor(hospital)40  0.682455   0.079230   8.614  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41 -0.595735   0.077561  -7.681 1.79e-14 ***\n#&gt; as.factor(hospital)42  0.432897   0.086529   5.003 5.78e-07 ***\n#&gt; as.factor(hospital)43 -1.206881   0.082348 -14.656  &lt; 2e-16 ***\n#&gt; as.factor(hospital)44 -0.438119   0.092421  -4.740 2.17e-06 ***\n#&gt; as.factor(hospital)45 -0.641833   0.077497  -8.282  &lt; 2e-16 ***\n#&gt; as.factor(hospital)46 -0.357058   0.083594  -4.271 1.97e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7512 on 7315 degrees of freedom\n#&gt; Multiple R-squared:  0.4973, Adjusted R-squared:  0.4938 \n#&gt; F-statistic: 139.2 on 52 and 7315 DF,  p-value: &lt; 2.2e-16\n\n\nThe difference is clearly seen by above code."
  },
  {
    "objectID": "content/01_journal/06_rct.html#part-1",
    "href": "content/01_journal/06_rct.html#part-1",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(readr)\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nabtest_data &lt;- readRDS(\"../../Causal_Data_Science_Data/abtest_online.rds\")\n\n# Part 1\nsummary(abtest_data)\n\n#&gt;       ip             chatbot        previous_visit   mobile_device  \n#&gt;  Length:1000        Mode :logical   Min.   : 0.000   Mode :logical  \n#&gt;  Class :character   FALSE:496       1st Qu.: 1.000   FALSE:683      \n#&gt;  Mode  :character   TRUE :504       Median : 1.000   TRUE :317      \n#&gt;                                     Mean   : 2.021                  \n#&gt;                                     3rd Qu.: 3.000                  \n#&gt;                                     Max.   :13.000                  \n#&gt;     purchase     purchase_amount\n#&gt;  Min.   :0.000   Min.   : 0.00  \n#&gt;  1st Qu.:0.000   1st Qu.: 0.00  \n#&gt;  Median :0.000   Median : 0.00  \n#&gt;  Mean   :0.381   Mean   :13.14  \n#&gt;  3rd Qu.:1.000   3rd Qu.:27.44  \n#&gt;  Max.   :1.000   Max.   :81.35\n\nggplot(abtest_data, aes(x = chatbot, y = previous_visit, fill = chatbot)) +\n  geom_boxplot() +\n  labs(title = \"Continuous covariate - previous_visit\",\n       x = \"Chatbot\", y = \"Previous Visits\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(abtest_data, aes(x = mobile_device, fill = chatbot)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Categorical covariate - mobile_device\",\n       x = \"Mobile Device\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(abtest_data, aes(x = as.factor(purchase), fill = chatbot)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Binary outcome - purchase\",\n       x = \"Purchase\", y = \"Count\") +\n  theme_minimal()"
  },
  {
    "objectID": "content/01_journal/06_rct.html#part-2",
    "href": "content/01_journal/06_rct.html#part-2",
    "title": "Randomized Controlled Trials",
    "section": "Part 2",
    "text": "Part 2\n\nmodel_sales &lt;- lm(purchase_amount ~ chatbot + mobile_device + previous_visit, data = abtest_data)\nsummary(model_sales)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot + mobile_device + previous_visit, \n#&gt;     data = abtest_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -25.414 -14.428  -8.435  12.559  64.584 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        15.2840     1.1227  13.614  &lt; 2e-16 ***\n#&gt; chatbotTRUE        -6.8488     1.1792  -5.808 8.49e-09 ***\n#&gt; mobile_deviceTRUE  -0.8562     1.2642  -0.677  0.49841    \n#&gt; previous_visit      0.7792     0.2869   2.716  0.00673 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.6 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.04243,    Adjusted R-squared:  0.03954 \n#&gt; F-statistic: 14.71 on 3 and 996 DF,  p-value: 2.228e-09"
  },
  {
    "objectID": "content/01_journal/06_rct.html#part-3",
    "href": "content/01_journal/06_rct.html#part-3",
    "title": "Randomized Controlled Trials",
    "section": "Part 3",
    "text": "Part 3\nInclude an interaction term for the subgroup (e.g., mobile users); chat bot interaction when using mobile device\n\nmodel_interaction &lt;- lm(purchase_amount ~ chatbot * mobile_device + previous_visit, data = abtest_data)\nsummary(model_interaction)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device + previous_visit, \n#&gt;     data = abtest_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -25.403 -14.450  -8.445  12.549  64.562 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                   15.27384    1.19130  12.821  &lt; 2e-16 ***\n#&gt; chatbotTRUE                   -6.82837    1.42580  -4.789 1.93e-06 ***\n#&gt; mobile_deviceTRUE             -0.82375    1.79309  -0.459  0.64605    \n#&gt; previous_visit                 0.77914    0.28711   2.714  0.00677 ** \n#&gt; chatbotTRUE:mobile_deviceTRUE -0.06455    2.52907  -0.026  0.97964    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.6 on 995 degrees of freedom\n#&gt; Multiple R-squared:  0.04243,    Adjusted R-squared:  0.03858 \n#&gt; F-statistic: 11.02 on 4 and 995 DF,  p-value: 9.485e-09\n\n# Compute the Conditional Average Treatment Effect (CATE) for mobile users\ncate_mobile &lt;- coef(model_interaction)[\"chatbotTRUE:mobile_deviceTRUE\"]\ncat(\"CATE for Mobile Users:\", cate_mobile, \"\\n\")\n\n#&gt; CATE for Mobile Users: -0.06455412\n\n\nThe CATE represents the additional effect of the chatbot for mobile users compared to non-mobile users.\nThat is, if the coefficient is statistically significant and positive, it suggests that the effect of the chat bot on sales is different for mobile users compared to non-mobile users.\nHere the effect of the chat bot on sales is not different for mobile users compared to non-mobile users."
  },
  {
    "objectID": "content/01_journal/06_rct.html#part-4",
    "href": "content/01_journal/06_rct.html#part-4",
    "title": "Randomized Controlled Trials",
    "section": "Part 4",
    "text": "Part 4\n\nmodel_logistic &lt;- glm(purchase ~ chatbot + mobile_device + previous_visit, data = abtest_data, family = \"binomial\")\nsummary(model_logistic)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot + mobile_device + previous_visit, \n#&gt;     family = \"binomial\", data = abtest_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)       -0.22185    0.12347  -1.797  0.07237 .  \n#&gt; chatbotTRUE       -0.96894    0.13564  -7.144 9.09e-13 ***\n#&gt; mobile_deviceTRUE -0.07119    0.14530  -0.490  0.62417    \n#&gt; previous_visit     0.10606    0.03262   3.252  0.00115 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1262.2  on 996  degrees of freedom\n#&gt; AIC: 1270.2\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# coefficient for chatbot\ncoef_chatbot &lt;- coef(model_logistic)[\"chatbotTRUE\"]\n\n# Exponentiate the coefficient to get the odds ratio\nodds_ratio &lt;- exp(coef_chatbot)\n\ncat(\"Odds Ratio for Chatbot:\", odds_ratio, \"\\n\")\n\n#&gt; Odds Ratio for Chatbot: 0.3794843\n\n\nIf odds_ratio is greater than 1, you can say that the introduction of the chatbot is associated with an increase in the odds of making a purchase, and vice versa.\nHere it has no effect on increasing the odds of making the purchase."
  }
]